{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb0d778-089d-4e52-af56-a57635c7dd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8f801-6127-4a26-9974-bf770f95f037",
   "metadata": {},
   "source": [
    "##### Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7d12e-8c7b-4e9a-9609-c458f22cc52b",
   "metadata": {},
   "source": [
    "Feature selection is the process of selecting a subset of relevant and informative features from a larger set of available features for use in machine learning algorithms. The aim is to reduce the dimensionality of the data and improve the accuracy and efficiency of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09b795a-ea99-4094-953e-88c0e7ea3fa6",
   "metadata": {},
   "source": [
    "There are several techniques of feature selection. Let's take a look into a two most popular techniques.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fd54fd-0aa0-41d2-b9d6-0024a7dbee74",
   "metadata": {},
   "source": [
    "##### Forward Feature Selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24fb008-f2fb-4983-b615-89dbbae0c4ba",
   "metadata": {},
   "source": [
    "Forward feature selection involves starting with an empty set of features and iteratively adding one feature at a time based on their individual performance in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ddcac5-98e4-4b54-8fa8-ffced1bf98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating custom datasets for testing\n",
    "X,y=make_classification(\n",
    "     n_samples=800,#total rows\n",
    "    n_features=10,# total column\n",
    "    \n",
    "    n_informative=2,#informative features\n",
    "    n_redundant=2,\n",
    "    random_state=42\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b252a0dd-91d5-4410-a286-2932438a2d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f407a66a-54ba-4b1c-9fd7-17cda7ae7645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected feature (forward): [9] Score:  0.88125\n",
      "Selected feature (forward): [9, 0] Score:  0.89375\n",
      "Selected feature (forward): [9, 0, 2] Score:  0.89375\n",
      "Selected feature (forward): [9, 0, 2, 8] Score:  0.90625\n",
      "Selected feature (forward): [9, 0, 2, 8, 1] Score:  0.90625\n",
      "Selected feature (forward): [9, 0, 2, 8, 1, 4] Score:  0.90625\n",
      "Selected feature (forward): [9, 0, 2, 8, 1, 4, 5] Score:  0.90625\n",
      "Selected feature (forward): [9, 0, 2, 8, 1, 4, 5, 3] Score:  0.9\n",
      "Selected feature (forward): [9, 0, 2, 8, 1, 4, 5, 3, 6] Score:  0.9\n",
      "Selected feature (forward): [9, 0, 2, 8, 1, 4, 5, 3, 6, 7] Score:  0.9\n"
     ]
    }
   ],
   "source": [
    "selected_features=[]\n",
    "for i in range(X_train.shape[1]):\n",
    "    best_acc=0\n",
    "    best_feature=None\n",
    "\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if j not in selected_features:\n",
    "            features=selected_features+[j]\n",
    "\n",
    "            model=LogisticRegression()\n",
    "            model.fit(X_train[:,features],y_train)\n",
    "            accuracy=model.score(X_test[:,features],y_test)\n",
    "\n",
    "            if accuracy>best_acc:\n",
    "                best_acc=accuracy\n",
    "                best_feature=j\n",
    "\n",
    "    selected_features.append(best_feature)\n",
    "    print(\"Selected feature (forward):\",selected_features,\"Score: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ccafef-070c-48bf-ad41-6be4b88f0e11",
   "metadata": {},
   "source": [
    "##### Backward Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b9fc3-b0a4-4658-9526-70bf48e97459",
   "metadata": {},
   "source": [
    "Backward feature selection, on the other hand, starts with all available features and iteratively removes one feature at a time based on their individual performance in predicting the outcome variable. This process continues until a stopping criterion is met, such as reaching a predefined number of features or a specific level of accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb32d7e2-0bc3-44a0-8a8e-01832e7efc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features (Backward): [1, 2, 3, 4, 5, 6, 7, 8, 9] Score: 0.9\n",
      "Selected Features (Backward): [2, 3, 4, 5, 6, 7, 8, 9] Score: 0.8875\n",
      "Selected Features (Backward): [3, 4, 5, 6, 7, 8, 9] Score: 0.8875\n",
      "Selected Features (Backward): [4, 5, 6, 7, 8, 9] Score: 0.89375\n",
      "Selected Features (Backward): [5, 6, 7, 8, 9] Score: 0.8875\n",
      "Selected Features (Backward): [6, 7, 8, 9] Score: 0.8875\n",
      "Selected Features (Backward): [6, 7, 8] Score: 0.4375\n",
      "Selected Features (Backward): [6, 8] Score: 0.5\n",
      "Selected Features (Backward): [8] Score: 0.45625\n"
     ]
    }
   ],
   "source": [
    "# Implement backward feature elimination\n",
    "selected_features = list(range(X_train.shape[1]))\n",
    "\n",
    "for i in range(X_train.shape[1] - 1):\n",
    "    \n",
    "    worst_accuracy = 1\n",
    "    worst_feature = None\n",
    "    \n",
    "    for j in selected_features:\n",
    "        \n",
    "        features = selected_features.copy()\n",
    "        features.remove(j)\n",
    "        \n",
    "        model = LogisticRegression()\n",
    "        model.fit(X_train[:, features], y_train)\n",
    "        accuracy = model.score(X_test[:, features], y_test)\n",
    "        \n",
    "        if accuracy < worst_accuracy:\n",
    "            worst_accuracy = accuracy\n",
    "            worst_feature = j\n",
    "            \n",
    "    selected_features.remove(worst_feature)\n",
    "    print(\"Selected Features (Backward):\", selected_features, \"Score:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75fbb47-d41d-494e-81ed-1ed9c4e60093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
